{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84894,"databundleVersionId":9709193,"sourceType":"competition"},{"sourceId":88758,"databundleVersionId":10176169,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:20.368958Z","iopub.execute_input":"2024-11-22T08:22:20.369424Z","iopub.status.idle":"2024-11-22T08:22:20.381141Z","shell.execute_reply.started":"2024-11-22T08:22:20.369387Z","shell.execute_reply":"2024-11-22T08:22:20.379992Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s4e10/sample_submission.csv\n/kaggle/input/playground-series-s4e10/train.csv\n/kaggle/input/playground-series-s4e10/test.csv\n/kaggle/input/loan-approval-prediction/sample_submission.csv\n/kaggle/input/loan-approval-prediction/train.csv\n/kaggle/input/loan-approval-prediction/test.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom scipy.stats import uniform\nfrom datetime import datetime\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:20.383549Z","iopub.execute_input":"2024-11-22T08:22:20.384032Z","iopub.status.idle":"2024-11-22T08:22:20.395281Z","shell.execute_reply.started":"2024-11-22T08:22:20.383980Z","shell.execute_reply":"2024-11-22T08:22:20.393968Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s4e10/train.csv')\ntest  = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s4e10/sample_submission.csv')\n\n# 삭제 전 ID 저장\ntrain_idx = train['id']\ntest_idx = test['id']\n\ntrain = train.drop(columns=['id'])\ntest = test.drop(columns=['id'])\n\n# 데이터에 파생변수1 추가\ndef cal_loan_mean_ratio(df):\n    \n    mean_lst = df.groupby('loan_grade')['loan_amnt'].mean()\n    df['loan_amnt_mean_ratio'] = df.apply(lambda x: x['loan_amnt'] / mean_lst[x['loan_grade']], axis = 1)\n    \n    return df\n\ntrain = cal_loan_mean_ratio(train)\ntest = cal_loan_mean_ratio(test)\n\n'''\n# 데이터에 파생변수 2 추가\n# 가중치 검사 및 검증 결과 유의미하지 않음\ndef int_rate_by_Credlength(df):\n    df['int_rate_by_Credlength'] = df['loan_int_rate'] / df['cb_person_cred_hist_length']\n    return df\n\ntrain = int_rate_by_Credlength(train)\ntest = int_rate_by_Credlength(test)\n'''\n\n# 데이터에 파생변수 3 추가\n# 유효하지 않은 열을 유효하게 -> 나이/재직기간을 범주화 하자.\n\ndef categorize_age(age):\n    if age < 20:\n        return 0\n    elif 20 <= age < 30:\n        return 1\n    elif 30 <= age < 40:\n        return 2\n    elif 40 <= age < 50:\n        return 3\n    else:\n        return 4\n\ntrain['age_to_categorial'] = train['person_age'].apply(categorize_age)\ntest['age_to_categorial'] = test['person_age'].apply(categorize_age)\n\n\n# 데이터에 파생변수 4 추가\n# 재직기간의 범주화\n'''\ndef categorize_emp(emp):\n    if emp < 3:\n        return 0\n    elif 3 <= emp < 6:\n        return 1\n    elif 6 <= emp < 10:\n        return 2\n    elif 10 <= emp < 20:\n        return 3\n    else:\n        return 4\n\ntrain['emp_to_categorial'] = train['person_emp_length'].apply(categorize_emp)\ntest['emp_to_categorial'] = test['person_emp_length'].apply(categorize_emp)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:20.396508Z","iopub.execute_input":"2024-11-22T08:22:20.396821Z","iopub.status.idle":"2024-11-22T08:22:21.781225Z","shell.execute_reply.started":"2024-11-22T08:22:20.396791Z","shell.execute_reply":"2024-11-22T08:22:21.780004Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"\"\\ndef categorize_emp(emp):\\n    if emp < 3:\\n        return 0\\n    elif 3 <= emp < 6:\\n        return 1\\n    elif 6 <= emp < 10:\\n        return 2\\n    elif 10 <= emp < 20:\\n        return 3\\n    else:\\n        return 4\\n\\ntrain['emp_to_categorial'] = train['person_emp_length'].apply(categorize_emp)\\ntest['emp_to_categorial'] = test['person_emp_length'].apply(categorize_emp)\\n\""},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# 결측치는 러프하게 평균으로 진행\n\ndef Na_mean_change(df, features):\n    \n    for col in features:\n        df[col].fillna(df[col].mean(), inplace = True)\n    \n    pd.DataFrame(df)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.783280Z","iopub.execute_input":"2024-11-22T08:22:21.783657Z","iopub.status.idle":"2024-11-22T08:22:21.789545Z","shell.execute_reply.started":"2024-11-22T08:22:21.783622Z","shell.execute_reply":"2024-11-22T08:22:21.788623Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# 라벨 인코더 적용\n\ndef preprocess_data(df_train, df_test):\n    label_enc = LabelEncoder()\n    \n    label_cols = ['person_home_ownership', 'loan_grade', 'cb_person_default_on_file']\n    \n    for col in label_cols:\n        df_train[col] = label_enc.fit_transform(df_train[col])\n        df_test[col] = label_enc.transform(df_test[col])\n        \n    df_train = pd.get_dummies(df_train, columns = ['loan_intent'], drop_first = True)\n    df_test = pd.get_dummies(df_test, columns = ['loan_intent'], drop_first = True)\n    \n    target_col = 'loan_status'\n    train_columns = df_train.drop(columns =[target_col]).columns\n    \n    df_test = df_test.reindex(columns = train_columns, fill_value = 0)\n    \n    return df_train, df_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.790761Z","iopub.execute_input":"2024-11-22T08:22:21.791128Z","iopub.status.idle":"2024-11-22T08:22:21.814539Z","shell.execute_reply.started":"2024-11-22T08:22:21.791093Z","shell.execute_reply":"2024-11-22T08:22:21.813242Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"train2 = train\ntest2 = test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.815668Z","iopub.execute_input":"2024-11-22T08:22:21.816016Z","iopub.status.idle":"2024-11-22T08:22:21.835081Z","shell.execute_reply.started":"2024-11-22T08:22:21.815980Z","shell.execute_reply":"2024-11-22T08:22:21.833763Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"Na_mean_change(train2, ['person_emp_length', 'loan_int_rate'])\ndf_test = Na_mean_change(test2, ['person_emp_length', 'loan_int_rate'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.836899Z","iopub.execute_input":"2024-11-22T08:22:21.837293Z","iopub.status.idle":"2024-11-22T08:22:21.858306Z","shell.execute_reply.started":"2024-11-22T08:22:21.837225Z","shell.execute_reply":"2024-11-22T08:22:21.856992Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/800587267.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[col].fillna(df[col].mean(), inplace = True)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"X = train2.drop(columns = 'loan_status', axis = 1).iloc[:,:-5]\ny = train2['loan_status']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.859821Z","iopub.execute_input":"2024-11-22T08:22:21.860215Z","iopub.status.idle":"2024-11-22T08:22:21.878385Z","shell.execute_reply.started":"2024-11-22T08:22:21.860177Z","shell.execute_reply":"2024-11-22T08:22:21.876801Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# Futher work begins here\n\n- use ML pipelines\n- use miltiple ML methods\n     - LightGBM, XGBoost, etc\n ","metadata":{}},{"cell_type":"code","source":"# separate num cols and car cols\n\ncat_cols = X.select_dtypes(exclude = np.number).columns.tolist()\nnum_cols = X.select_dtypes(include = np.number).columns.tolist()\n\nnum_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.881829Z","iopub.execute_input":"2024-11-22T08:22:21.882231Z","iopub.status.idle":"2024-11-22T08:22:21.892483Z","shell.execute_reply.started":"2024-11-22T08:22:21.882194Z","shell.execute_reply":"2024-11-22T08:22:21.891351Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"['person_age',\n 'person_income',\n 'person_emp_length',\n 'loan_amnt',\n 'loan_int_rate']"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# 훈련 - 테스트 데이터 분리\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = 0.2, random_state = 42\n)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.894006Z","iopub.execute_input":"2024-11-22T08:22:21.894437Z","iopub.status.idle":"2024-11-22T08:22:21.921036Z","shell.execute_reply.started":"2024-11-22T08:22:21.894403Z","shell.execute_reply":"2024-11-22T08:22:21.918234Z"}},"outputs":[{"name":"stdout","text":"(46916, 8) (46916,) (11729, 8) (11729,)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# 훈련 - 검증 데이터 분리\n\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X_train, y_train, test_size = 0.2, random_state = 42\n)\n\nprint(X_tr.shape, X_val.shape, y_tr.shape, y_val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.922748Z","iopub.execute_input":"2024-11-22T08:22:21.923140Z","iopub.status.idle":"2024-11-22T08:22:21.940234Z","shell.execute_reply.started":"2024-11-22T08:22:21.923101Z","shell.execute_reply":"2024-11-22T08:22:21.939077Z"}},"outputs":[{"name":"stdout","text":"(37532, 8) (9384, 8) (37532,) (9384,)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer # 결측치 처리 관련\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# StratifiedKFold : 분류 작업 할 때\n# KFold : 수치 작업 할 때\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold, KFold\n\nimport numpy as np\nfrom scipy.stats import uniform, randint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.941723Z","iopub.execute_input":"2024-11-22T08:22:21.942091Z","iopub.status.idle":"2024-11-22T08:22:21.949438Z","shell.execute_reply.started":"2024-11-22T08:22:21.942055Z","shell.execute_reply":"2024-11-22T08:22:21.948350Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# 파이프라인 구축\n\n# 연속형/범주형 피처 변환기 설정\n\ncategorical_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\npreprocessor = ColumnTransformer( # 최종 전처리\n    transformers=[\n        ('num', numerical_transformer, num_cols), # 이름, 처리 방법, 대상 순\n        ('cat', categorical_transformer, cat_cols)\n    ])\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor), #전처리 라인 적용\n    ('classifier', RandomForestClassifier(random_state=42)) #분류는 포레스트 적용\n])\n\nparam_distributions = { #파라미터 설정\n    'classifier__min_impurity_decrease': uniform(0.0001, 0.001),\n    'classifier__max_depth': randint(20, 50),\n    'classifier__min_samples_split': randint(2, 25),\n    'classifier__min_samples_leaf': randint(1, 25),\n}\n\nsplit_number = 5  # fold숫자 정하기\nstratified_kfold = StratifiedKFold(n_splits=split_number, shuffle=True, random_state=42)\n\nrandom_search = RandomizedSearchCV( #랜덤서치 실행\n    estimator=pipeline, #원래는 Tree, REG같은 모델이 들어갔던 자리, 전처리와 모델처리가 한번에 진행됨.\n    param_distributions=param_distributions,\n    n_iter=50, #시도횟수\n    cv= stratified_kfold,\n    scoring='roc_auc', # 평가 지표\n    random_state=42,\n    n_jobs=-1 #가용한 모든 코어 이용\n)\n\nrandom_search.fit(X_train, y_train)\n\n# 검증 세트에 대한 예측 확률 계산\ny_val_probs = random_search.predict_proba(X_val)[:, 1]  # 클래스 1(양성 클래스)에 대한 확률 추출\n\n# ROC-AUC 점수 계산 및 출력\nroc_auc = roc_auc_score(y_val, y_val_probs)\nprint(f'ROC-AUC 점수: {roc_auc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T08:22:21.950736Z","iopub.execute_input":"2024-11-22T08:22:21.951090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#LightGBM 적용\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, randint\n\n# LightGBM이 적용된 파이프라인\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),  # 전처리 라인 적용\n    ('classifier', LGBMClassifier(random_state=42))  # 분류는 LightGBM 적용\n])\n\n# LightGBM 파라미터 설정\nparam_distributions = {\n    'classifier__learning_rate': uniform(0.01, 0.1),  # 학습률\n    'classifier__n_estimators': randint(50, 500),  # 트리의 개수\n    'classifier__max_depth': randint(3, 15),  # 트리의 최대 깊이\n    'classifier__num_leaves': randint(20, 50),  # 리프 노드의 최대 개수\n    'classifier__min_child_samples': randint(10, 50),  # 리프 노드의 최소 샘플 수\n    'classifier__subsample': uniform(0.6, 0.4),  # 데이터 샘플링 비율\n    'classifier__colsample_bytree': uniform(0.6, 0.4),  # 컬럼 샘플링 비율\n    'classifier__reg_alpha': uniform(0.0, 0.1),  # L1 정규화\n    'classifier__reg_lambda': uniform(0.0, 0.1),  # L2 정규화\n}\n\n# RandomizedSearchCV를 사용한 하이퍼파라미터 튜닝\nrandom_search_2 = RandomizedSearchCV(\n    estimator=pipeline,\n    param_distributions=param_distributions,\n    n_iter=50,  # 시도 횟수\n    cv=stratified_kfold,  # 교차 검증\n    scoring='roc_auc',  # 평가 지표\n    random_state=42,\n    n_jobs=-1  # 가용한 모든 코어 사용\n)\n\nrandom_search_2.fit(X_train, y_train)\n\n# 검증 세트에 대한 예측 확률 계산\ny_val_probs = random_search_2.predict_proba(X_val)[:, 1]  # 클래스 1(양성 클래스)에 대한 확률 추출\n\n# ROC-AUC 점수 계산 및 출력\nroc_auc = roc_auc_score(y_val, y_val_probs)\nprint(f'ROC-AUC 점수: {roc_auc:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport numpy as np\n\ndef get_score(model, X_tr, X_val, y_tr, y_val):\n    tr_pred = model.predict_proba(X_tr)[:, 1]\n    val_pred = model.predict_proba(X_val)[:, 1]\n    tr_score = np.sqrt(roc_auc_score(y_tr, tr_pred))\n    val_score = np.sqrt(roc_auc_score(y_val, val_pred))\n    return f\"train: {tr_score}, validation: {val_score}\"\n\nbest_model = random_search.best_estimator_\nget_score(best_model, X_tr, X_val, y_tr, y_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model = random_search_2.best_estimator_\nget_score(best_model, X_tr, X_val, y_tr, y_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_preds = best_model.predict(X_test)\nresult = pd.DataFrame({'Approval': final_preds})\nresult.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Ensombling - 3models\n- Tree, XGBoost, LightGBM 3 model ensemble","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# 데이터 준비\n\n# 수치형 데이터 전처리\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# 범주형 데이터 전처리\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# 전처리기 정의\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_cols),\n        ('cat', categorical_transformer, cat_cols)\n    ]\n)\n\n# 모델별 파이프라인 정의\nmodels = {\n    \"LightGBM\": LGBMClassifier(random_state=42),\n    \"RandomForestRegressor\": RandomForestRegressor(random_state=42),\n    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n}\n\n# 평가 결과 저장\nresults = {}\n\n# 데이터 분할\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# 파이프라인 생성 및 모델 학습\nfor model_name, model in models.items():\n    print(f\"\\n모델: {model_name}\")\n    \n    # 파이프라인 생성\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', model)\n    ])\n    \n    # 교차 검증을 통한 점수 계산\n    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=stratified_kfold, scoring='roc_auc', n_jobs=-1)\n    print(f\"교차 검증 평균 ROC-AUC 점수: {np.mean(cv_scores):.4f}\")\n    \n    # 학습 및 검증\n    pipeline.fit(X_train, y_train)\n    y_val_probs = pipeline.predict_proba(X_val)[:, 1]  # 클래스 1에 대한 확률 추출\n    roc_auc = roc_auc_score(y_val, y_val_probs)\n    print(f\"검증 세트 ROC-AUC 점수: {roc_auc:.4f}\")\n    \n    # 결과 저장\n    results[model_name] = {\n        'CV ROC-AUC': np.mean(cv_scores),\n        'Validation ROC-AUC': roc_auc\n    }\n\n# 최종 결과 출력\nprint(\"\\n모델별 평가 결과:\")\nfor model_name, scores in results.items():\n    print(f\"{model_name}:\")\n    print(f\"  - 교차 검증 평균 ROC-AUC: {scores['CV ROC-AUC']:.4f}\")\n    print(f\"  - 검증 세트 ROC-AUC: {scores['Validation ROC-AUC']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_score(model, X_tr, X_val, y_tr, y_val):\n    \"\"\"\n    주어진 모델에 대해 훈련 및 검증 데이터에서 ROC-AUC 점수를 계산합니다.\n    \"\"\"\n    # 예측 확률 계산\n    tr_pred = model.predict_proba(X_tr)[:, 1]\n    val_pred = model.predict_proba(X_val)[:, 1]\n    \n    # 점수 계산\n    tr_score = np.sqrt(roc_auc_score(y_tr, tr_pred))\n    val_score = np.sqrt(roc_auc_score(y_val, val_pred))\n    \n    return f\"Train Score: {tr_score:.4f}, Validation Score: {val_score:.4f}\"\n\n# 최적 모델에 대해 점수 출력\nfor model_name, model_pipeline in models.items():\n    print(f\"\\n모델: {model_name}\")\n    \n    # 전처리를 포함한 파이프라인으로 학습\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', model_pipeline)\n    ])\n    pipeline.fit(X_train, y_train)  # 데이터 학습\n    \n    # 성능 평가\n    try:\n        print(get_score(pipeline, X_train, X_val, y_train, y_val))\n    except AttributeError as e:\n        print(f\"모델 {model_name}은 'predict_proba'를 지원하지 않습니다.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}