{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LightGBM 방식","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm.callback import early_stopping\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# For binary classification (use only two classes for simplicity in this example)\nX = X[y != 2]\ny = y[y != 2]\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create LightGBM Dataset\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n# Define parameters\nparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"device\": \"gpu\",  # Use GPU for acceleration\n}\n\n# Add early stopping as a callback\ncallbacks = [early_stopping(stopping_rounds=10)]\n\n# Train LightGBM model\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=100,\n    valid_sets=[train_data, test_data],\n    callbacks=callbacks,  # Use callbacks for early stopping\n)\n\n# Make predictions\ny_pred = model.predict(X_test)\ny_pred_binary = np.round(y_pred)  # Convert probabilities to binary predictions\n\n# Evaluate model\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:35:39.271332Z","iopub.execute_input":"2024-12-04T07:35:39.271667Z","iopub.status.idle":"2024-12-04T07:35:39.275840Z","shell.execute_reply.started":"2024-12-04T07:35:39.271639Z","shell.execute_reply":"2024-12-04T07:35:39.274911Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# CatBoost GPU","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load the Seaborn tips dataset\ntips = sns.load_dataset('tips')\n\n# Convert categorical columns to string for CatBoost compatibility\ntips['sex'] = tips['sex'].astype(str)\ntips['smoker'] = tips['smoker'].astype(str)\ntips['day'] = tips['day'].astype(str)\ntips['time'] = tips['time'].astype(str)\n\n# Define features and target\nX = tips.drop(columns=['total_bill'])\ny = tips['total_bill']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical columns for CatBoost\ncategorical_features = ['sex', 'smoker', 'day', 'time']\n\n# Create Pool objects for CatBoost\ntrain_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features)\ntest_pool = Pool(data=X_test, label=y_test, cat_features=categorical_features)\n\n# Initialize the CatBoostRegressor with GPU settings\nmodel = CatBoostRegressor(\n    iterations=500,\n    learning_rate=0.1,\n    depth=6,\n    loss_function='RMSE',\n    task_type=\"GPU\",  # Use GPU\n    devices='0'       # Specify GPU device\n)\n\n# Train the model\nmodel.fit(train_pool, eval_set=test_pool, verbose=50, early_stopping_rounds=10)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(f\"RMSE: {rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:41:11.341712Z","iopub.execute_input":"2024-12-04T07:41:11.342063Z","iopub.status.idle":"2024-12-04T07:41:36.106169Z","shell.execute_reply.started":"2024-12-04T07:41:11.342035Z","shell.execute_reply":"2024-12-04T07:41:36.105185Z"}},"outputs":[{"name":"stdout","text":"0:\tlearn: 8.3682067\ttest: 9.1091529\tbest: 9.1091529 (0)\ttotal: 4.27s\tremaining: 35m 30s\n50:\tlearn: 5.0242698\ttest: 6.2593988\tbest: 6.2593988 (50)\ttotal: 4.85s\tremaining: 42.7s\nbestTest = 6.175403441\nbestIteration = 71\nShrink model to first 72 iterations.\nRMSE: 6.1754\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# XGBooost GPU","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\n\n# Load the Seaborn tips dataset\ntips = sns.load_dataset('tips')\n\n# Preprocess the dataset\n# Convert categorical variables into dummy/one-hot encoding\ntips = pd.get_dummies(tips, columns=['sex', 'smoker', 'day', 'time'], drop_first=True)\n\n# Define features and target\nX = tips.drop(columns=['total_bill'])\ny = tips['total_bill']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert the data into DMatrix format, which is optimized for XGBoost\ntrain_data = xgb.DMatrix(X_train, label=y_train)\ntest_data = xgb.DMatrix(X_test, label=y_test)\n\n# Define GPU parameters for XGBoost\nparams = {\n    \"objective\": \"reg:squarederror\",  # Regression objective\n    \"eval_metric\": \"rmse\",           # Evaluation metric\n    \"tree_method\": \"hist\",       # Use GPU for histogram building\n    \"device\" : \"cuda\",\n    \"learning_rate\": 0.1,\n    \"max_depth\": 6\n}\n\n# Train the XGBoost model\nmodel = xgb.train(\n    params,\n    train_data,\n    num_boost_round=200,\n    evals=[(train_data, \"train\"), (test_data, \"test\")],\n    early_stopping_rounds=10,\n    verbose_eval=10\n)\n\n# Make predictions\ny_pred = model.predict(test_data)\n\n# Evaluate model performance\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(f\"RMSE: {rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:44:14.516737Z","iopub.execute_input":"2024-12-04T07:44:14.517078Z","iopub.status.idle":"2024-12-04T07:44:14.691939Z","shell.execute_reply.started":"2024-12-04T07:44:14.517049Z","shell.execute_reply":"2024-12-04T07:44:14.691135Z"}},"outputs":[{"name":"stdout","text":"[0]\ttrain-rmse:8.16107\ttest-rmse:9.02373\n[10]\ttrain-rmse:4.75365\ttest-rmse:7.12777\n[20]\ttrain-rmse:3.46133\ttest-rmse:6.63339\n[30]\ttrain-rmse:2.83794\ttest-rmse:6.62786\n[37]\ttrain-rmse:2.62125\ttest-rmse:6.60553\nRMSE: 6.6142\n","output_type":"stream"}],"execution_count":14}]}