{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ch11_the_NLP_lectures","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOYftq6bCf/nJoT/HbO2hR0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wXHzrpasPdBt"},"source":["## 사전 학습의 개념\n","- 사전 학습 모델이란 기존에 자비어(Xavier) 등 임의의 값으로 초기화된 모델의 가중치들을 다른 문제(task)에 학습시킨 가중치들로 초기화하는 방법이다. \n","- 이미지 분류에서는 보통 전이학습이라는 용어를 사용하기도 했다. \n","- 자연어에서의 가장 대표적인 사전학습 모델이 버트와 GPT이다. \n","- 현재는 이러한 대부분의 자연어 처리 모델이 언어 모델을 사전 학습한 모델을 활용하도록 한다. \n","  + 예를 들면, `오늘 저녁 반찬 간이 조금 싱겁다`라는 문장이 있을 때, `오늘 아침 반찬 간이`라는 단어들을 통해 `싱거워`라는 단어를 모델이 예측하며 학습하게 된다.  \n","- 이러한 학습을 통해 모델은 언어에 대한 전반적인 이해(Natural Language Understanding, NLU)를 하게 되고, 이렇게 사전 학습된 지식을 기반으로 하위 문제에 대한 성능을 향상 시킨다. "]},{"cell_type":"markdown","metadata":{"id":"ADB1Zy__RUOa"},"source":["### 사전 학습의 방법\n","- 첫번째는 특징 기반(feature-based) 방법이다. \n","- 특징 기반 방법이란 사전 학습된 특징을 하위 문제의 모델에 부가적인 특징을 활용하는 방법이다. \n","  + 특징 기반의 사전 학습 활용 방법의 대표적인 예는 `word2vec`으로, 학습한 임베딩 특징을 우리가 학습하고자 하는 모델의 임베딩 특징으로 활용하는 방법이다. \n","  + 사전 학습한 가중치를 활용하는 또 다른 방법은 미세 조정(`fine-tuning`)이다. 미세 조정이란 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해서 모델을 추가로 학습(미세 조정) 하는 방법을 말한다. \n"]},{"cell_type":"markdown","metadata":{"id":"_v7K_lDBZjLB"},"source":["## 기존연구 소개\n","- 버트와 GPT를 배우기에 앞서 자연어 처리 연구의 흐름에 대해 살펴보도록 한다. \n"]},{"cell_type":"markdown","metadata":{"id":"tLFJflmYhGJF"},"source":["### Word2Vec & Skip Gram\n","- 문장에서 특정한 단어가 어떻게 올 것인지 예측하는 방법의 가장 기본적인 원리라고 할 수 있다. \n","- word2vec은 CBOW(Continuous Bag of Words)와 Skip-Gram이라는 두가지 모델로 나뉜다. \n","- 두 모델은 서로 반대되는 개념이라고 할 수 있다. "]},{"cell_type":"code","metadata":{"id":"A0-3jXOfcgCN","colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"status":"ok","timestamp":1608257843425,"user_tz":-540,"elapsed":1684,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"e99ee8b4-292e-4a76-9066-e88c497d6939"},"source":["from IPython.display import HTML\n","\n","HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/sY4YyacSsLc?start=596\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/sY4YyacSsLc?start=596\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"3nsyOKaYGPfi","executionInfo":{"status":"ok","timestamp":1608165029923,"user_tz":-540,"elapsed":1141,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"6eddd0ef-78c1-4148-d117-b130e82c3c00"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UqRCEmrv1gQ?start=596\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UqRCEmrv1gQ?start=596\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"9RXOl-o4gwuH"},"source":["- 다음 문장을 확인해보자. 예시를 들면 다음과 같다. \n","  + 지훈은 냉장고에서 장어를 꺼내서 먹었다. \n","- CBOW는 주변 단어를 통해 하나의 단어를 예측하는 모델이다. \n","  + 지훈은 ___에서 장어를 꺼내서 먹었디. \n","- 반대로 Skip-Gram은 하나의 단어를 가지고 주변에 올 단어를 예측하는 모델이다. \n","  + ___ 냉장고__ ___  ___ ___.\n"]},{"cell_type":"markdown","metadata":{"id":"-DTkEovuHDIF"},"source":["- `CBOW`의 학습 방법은 다음과 같다. \n","  + 각 주변 단어들을 원-핫 벡터로 만들어 입력값으로 사용한다. \n","  + 가중치 행렬(weight matrix)을 각 원-핫 벡터에 곱해서 n-차원 벡터를 만든다(N-차원 은닉층)\n","  + 만들어진 n-차원 벡터를 모두 더한 후 개수로 나눠 평균 n-차원 벡터를 만든다(출력층 벡터)\n","  + N-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다. \n","  + 만들어진 벡터를 실제 예측하려고 하는 단어의 원-핫 벡터와 비교해서 학습한다. \n","- `Skip-Gram`의 학습 방법도 비슷한 과정으로 진행한다. 전체 과정은 다음과 같다. \n","  + 하나의 단어를 원-핫 벡터로 만들어서 입력값으로 사용한다. (입력층 벡터)\n","  + 가중치 행렬을 원-핫 벡터에 곱해서 n-차원 벡터를 만든다(N-차원 은닉층). \n","  + N-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다. \n","  + 만들어진 벡터를 실제 예측하려는 주변 단어들 각각의 원-핫 벡터와 비교해서 학습한다. \n","- 위 두개의 개념이 담긴 실습 예제는 다음 링크에서 확인하시기를 바랍니다. \n","  + https://wikidocs.net/50739\n","- 위 두개의 개념이 포함된 `Glove`라는 개념도 기억하면 좋다. \n"]},{"cell_type":"markdown","metadata":{"id":"MwzPRzIgd91_"},"source":["### 딥러닝 RNN & LSTM\n","- 순환 신경망은 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. \n","- 자연어처리에서 RNN은 한 단어에 대한 정보를 입력하면 이 단어 다음에 나올 단어를 맞추는 모델이다. \n","- RNN에 대한 구체적인 정보는 아래 유투브를 확인하도록 한다. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"jVVswEXGfj5S","executionInfo":{"status":"ok","timestamp":1608171628153,"user_tz":-540,"elapsed":1148,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"fc3b0a66-1310-4e66-ab59-78fcb0548782"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PahF2hZM6cs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/PahF2hZM6cs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"gEpEje0lf5Vi"},"source":["- RNN의 가장 큰 단점은 은닉정보가 많으면 많을수록 정보의 손실이 발생한다는 점이다. 이러한 RNN의 단점을 극복한 이론이 LSTM이라고 보면 된다. \n","- LSTM에 관한 구체적인 설명은 다음 유투브에서 확인하도록 한다. \n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"WxW4I6tVrY1o","executionInfo":{"status":"ok","timestamp":1608174732511,"user_tz":-540,"elapsed":1108,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"8d1f7c33-4c54-4f14-fea8-cb877775e8a6"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bX6GLbpw-A4\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bX6GLbpw-A4\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"kb3Nv9n8rvUB"},"source":["- RNN을 활용한 다양한 텍스트 분류 예제는 다음 링크에서 확인하도록 한다. \n","  + https://wikidocs.net/22891"]},{"cell_type":"markdown","metadata":{"id":"nAcAk_XmrqYY"},"source":["### 시퀀스 투 시퀀스 모델 & Attention\n","- 앞의 두 모형이 개별적인 단어들을 예측하는 모형이라고 한다면, 이번 모델은 문장을 출력하는 구조가 된다. \n","- 시퀀스 형태의 입력값을 시퀀스 형태의 출력으로 만들 수 있게 하는 모델로써, 하나의 텍스트 문장이 입력으로 들어오면 하나의 텍스트 문장을 출력하는 구조가 된다. \n","- 이 모델은 기계번역, 텍스트 요약, 이미지 설명, 대화 모델 등 다양한 분야에서 활용되고 있다.\n","- 그러나, 이 모델 역시, 순환망 특유의 문제인 장기 의존성 문제가 발생하기 때문에 이러한 단점을 극복하기 위해 나온 모델이 `Attention`이라고 하면 된다. \n","- 두 이론에 대해서는 영상을 통해서 학습을 하도록 한다. \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"ZLxg7ENCs7w7","executionInfo":{"status":"ok","timestamp":1608175096955,"user_tz":-540,"elapsed":930,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"3a49fc47-d2c6-4e72-da70-50b0a44cb4bb"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/WsQLdu2JMgI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/WsQLdu2JMgI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"cbOcVZZWuDuk"},"source":["- 마찬가지로 예제코드는 다음 코드에서 확인을 하면 된다. \n","  + https://wikidocs.net/24996 (Seq2Seq)\n","  + https://wikidocs.net/22893 (Attentino)"]},{"cell_type":"markdown","metadata":{"id":"x5PIsHUkvGtL"},"source":["### 트랜스포머\n","- 기존에는 이러한 순환 신경망을 기반으로 모델을 만들어 사용해왔다. 그러나 구글이 2017년에 `Attention is all you need` 논문을 발표하면서, RNN & CNN을 기반으로 한 모델과 다르게 단순히 어텐션 구조만으로 전체 모델을 만들어 어텐션 기법의 중요성을 강조한다. \n","  + https://arxiv.org/abs/1706.03762"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"7x7tZLTnDvnJ","executionInfo":{"status":"ok","timestamp":1608185572739,"user_tz":-540,"elapsed":1142,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"6ed95606-39c3-46f1-bb20-e2cefa1ebc4f"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mxGCEWOxfe8\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mxGCEWOxfe8\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"O1gzHh-ZU-qI"},"source":["- 기존 순환 신경망 모델은 시퀀스 순서에 따른 패턴을 보는 것이 중요하다. \n","  + \"나는 어제 기분이 좋았어\"라는 문장을 시퀀스 투 시퀀스를 거쳐 \"기분이 좋다니 저도 좋아요\"라고 문장을 만든다고 할 때, 순환 신경망의 경우 은닉 상태 벡터에 \"기분이 좋다\"는 문장의 맥락 정보가 반영되어 디코더에서 응답 문장을 생성할 수 있다. \n","  + 그런데, 큰 단점은 하나의 벡터에 인코더 부분에 해당하는 문장에 대한 모든 정보를 담고 있어, 문장 안의 개별 단어와의 관계를 파악하기 어려웠고, 문장의 길이가 길수록 모든 정보를 하나의 벡터에 포함하기에는 부족한 단점이 있었다. \n","- 쉼표가 다수 포함된, 즉 문장의 길이가 길수록 맨 마지막 문장과 첫번째 문장간의 유의미한 관계가 약해진다는 단점이 존재한다. \n","  + 이런 문제점을 극복하는 것이 트랜스포머 모델이라고 보면 된다. \n","- 이 때 사용되는 주요 모델이 `Self-Attention`이라는 기법이다. \n","- 트랜스포머 모델을 활용하여, 자동 챗봇을 구현하는 주요 기법이 된다. \n","  + 한국어 챗봇을 구현하는 실습은 다음 링크에서 확인하도록 한다. \n","  + https://wikidocs.net/89786\n","\n","\n","  "]},{"cell_type":"markdown","metadata":{"id":"ULbDOupsadT1"},"source":["## 사전학습 모델의 활용 - BERT\n","- 2018년 구글에서 공개한 논문이다. \n","  + 비지도 사전 학습을 한 모델에 추가로 하나의 완전 연결 계층만 추가한 후 미세 조정을 통해 총 11개의 자연어 처리 문제에서 최고의 성능을 보여줬다. \n","- 버트의 경우 기존에 나온 ELMo, OpenAI GPT에 비교해서, 양방향성을 띤다는 점이 두 모델과의 차이점이다. \n","  + 자세한 설명은 버트의 강의와 PDF 자료를 통해서 재 학습을 하도록 한다. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"lIlme8GqfyqI","executionInfo":{"status":"ok","timestamp":1608188419272,"user_tz":-540,"elapsed":974,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"f98b0bd4-c333-4897-e013-a07710d5d0f8"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IwtexRHoWG0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IwtexRHoWG0\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"URn6fsDqf_tv"},"source":["## 사전학습 모델의 활용 - GPT1\n","- 2018년 OpenAI에서 GPT 모델(Improving Language Understanding By Generative Pre-Training)을 처음 제안하였다. \n","- 버트보다 앞서 사전 학습 기법을 활용해 여러 문제에서 당시의 기존 모델들보다 높은 성능을 보여주고 있다. \n","  + 버트와의 차이점은 버트가 트랜스포머의 인코더 구조만 사용한 반면 GPT1에서는 트랜스포머의 디코더 구조만 사용했다. \n","  + 구체적인 내용은 강의를 통해서 확인한다. "]},{"cell_type":"code","metadata":{"id":"dFHUho7RcUw7","colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"status":"ok","timestamp":1608257851366,"user_tz":-540,"elapsed":1744,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"6e728bae-4098-4e3e-bd91-571fafcd8ab7"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/o_Wl29aW5XM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/o_Wl29aW5XM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"f18zyjLAo13r"},"source":["### 사전학습 모델의 활용 - GPT2\n","- GPT2 (Language Models are Unsupervised Multitask Learners\n",")의 모델 구조와 GPT1은 대부분 동일하다. \n","- 기존의 디코더에서 각 레이어 직후 레지듀얼 커넥션과 함께 적용되던 레이어 노멀라이제이션이 각 부분 블록의 입력 쪽으로 위치가 이동했고, 추가로 셀프 어텐션 레이어 이후에 레이어 노멀라이제이션이 적용된다. \n","- 자세한 설명은 마찬가지로 강의와 PDF를 통해서 배우도록 한다. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"70n77kK2pyxu","executionInfo":{"status":"ok","timestamp":1608258127333,"user_tz":-540,"elapsed":989,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"83d2938d-edaf-42f8-e67f-9ddececb44c6"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8hd2Q-3-BsQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8hd2Q-3-BsQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"bUKN8X-Zp4-r"},"source":["## 사전학습 모델의 활용 - GPT3\n","- 2020년, GPT3는 기존와 문제와는 전혀 다른 해법을 내놓았다. \n"]},{"cell_type":"markdown","metadata":{"id":"l9j88suhq7NS"},"source":["### 기존 사전 학습 모델의 문제점\n","- GPT3(Language Models are Few-Shot Learners) 이전의 모든 모델은 태스크와 무관한 representation을 학습하는 방향으로 발전해왔다. \n","- ELMo의 경우 결국에는 RNN 레이어를 쌓아 문맥 벡터를 만든다거나, \n","- 최근에는 BERT, GPT, ULMFit과 같이 트랜스포머 구조를 이용해 문맥을 표현하는 깊은 모델 \n","- 그런데, 이러한 모델의 문제점은 태스크와 상관없이 대량의 코퍼스를 이용해 학습된 모델은 fine-tuning을 통해 성공적인 퍼포먼스를 달성했다는 것이다. 직, 태스크에 따라 매번 미세 조정이 필요하다라는 한계가 있었다. \n"]},{"cell_type":"markdown","metadata":{"id":"9cu1lbkirDdY"},"source":["### GPT3의 개요\n","- GPT3은 인간의 학습 능력을 모방하려고 애를 썼다. \n","  + 학습 데이터가 많이 없어도 쉽게 추론할 수 있는 능력을 말한다. \n","- 이는 매우 중요하다. 몇가지 예시만 보고 태스크에 적응하여 문제를 푸는 일종의 `few shot learning`으로 각광을 받고 있기 때문에. 특히, 자연어에서는 더더욱 어려운 문제이기도 하다. \n","- 중요한 포인트는 다음과 같다. \n","  + 새로운 문제를 풀 때마다 많은 라벨링된 데이터가 필요하지만, 이제는 그럴 필요가 없어졌다. \n","  + 사람을 생각해보자. 대부분의 언어 테스크를 하기 위해 많은 예제 데이터를 필요로 하지 않는다. 즉, NLP의 유연성과 일반성을 가지게 되었다. \n","- 또 하나의 포인트는 모델의 사이즈를 크게 늘리는 것이다. \n","  + GPT-1: 1억개\n","  + BERT : 3억개\n","  + GTP-2: 14억개\n","  + ...\n","  + GTP-3: 1750억개 \n","- 논문에서는 이러한 정말로 큰 모델의 사이즈를 가지고, 24개 NLP 데이터셋을 가지고 3가지 조건 하에 모델 성능을 측정한다. \n","  + few-shot learning (in-context learning)\n","  + one-shot learning\n","  + zero-shot learning\n","- 실제로 최근들어서 다양한 논문들은 이러한 3가지 조건 하에서 개발할 수 있는 다양한 모델 성능을 측정하는 데 포커스를 둔다. \n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"ZD7WmEfju_za","executionInfo":{"status":"ok","timestamp":1608259474895,"user_tz":-540,"elapsed":1278,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"6d0fc36e-91a4-449b-8e7b-4e75d6b155c4"},"source":["HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/p24JUVgDkQk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/p24JUVgDkQk\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"PDMD2zNhAsQX","executionInfo":{"status":"ok","timestamp":1608297747359,"user_tz":-540,"elapsed":833,"user":{"displayName":"Ji-hoon Jung","photoUrl":"","userId":"03169308685755834042"}},"outputId":"0a127662-56a6-427d-ad9b-b4c20da186a6"},"source":["from IPython.display import HTML\n","HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XxJ5yGrsSIw\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XxJ5yGrsSIw\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"mu2c85SXo1Yw"},"source":["## 결론\n","- 지금까지 본 이론을 다 암기해야 하거나 기억해야 할 필요는 없다. \n","- 다만 향후 이론의 흐름을 이해하는 것은 매우 중요한데, 그 이유는 개발 및 활용의 관점에서, 어떤 튜토리얼을 배워야 하는지에 대한 중요한 가이드라인이 될 수 있기 때문에 그렇다. \n","- 이제 BERT와 GPT를 활용하여 실습을 진행해본다. "]}]}