{"cells":[{"cell_type":"markdown","metadata":{},"source":["### 트랜스포머를 이용한 주가 방향성 예측\n","\n","이전 장에서 머신러닝을 이용한 주가 방향성 예측 실습을 진행했다. 이번 장에서는 딥러닝, 특히 트랜스포머 모델을 활용한 주가 방향성 예측 실습을 진행한다. 트랜스포머 모델은 자연어 처리에서 큰 성과를 거두었지만, 시계열 데이터에서도 효과적으로 사용될 수 있다."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ta in c:\\users\\campus3s043\\desktop\\alpaco_lectures\\venv\\lib\\site-packages (0.11.0)\n","Requirement already satisfied: numpy in c:\\users\\campus3s043\\desktop\\alpaco_lectures\\venv\\lib\\site-packages (from ta) (1.26.4)\n","Requirement already satisfied: pandas in c:\\users\\campus3s043\\desktop\\alpaco_lectures\\venv\\lib\\site-packages (from ta) (2.2.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\campus3s043\\desktop\\alpaco_lectures\\venv\\lib\\site-packages (from pandas->ta) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\campus3s043\\desktop\\alpaco_lectures\\venv\\lib\\site-packages (from pandas->ta) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\campus3s043\\desktop\\alpaco_lectures\\venv\\lib\\site-packages (from pandas->ta) (2024.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\campus3s043\\desktop\\alpaco_lectures\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.2 -> 24.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install ta"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-15T05:47:27.975498Z","iopub.status.busy":"2024-05-15T05:47:27.974672Z","iopub.status.idle":"2024-05-15T05:47:27.990705Z","shell.execute_reply":"2024-05-15T05:47:27.989755Z","shell.execute_reply.started":"2024-05-15T05:47:27.975455Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import yfinance as yf # 없다면 !pip install yfinance 명령어로 설치\n","import ta  # 없다면 !pip install ta 명령어로 설치\n"]},{"cell_type":"markdown","metadata":{},"source":["먼저, 필요한 라이브러리를 불러오고 데이터를 준비한다.\n","훈련 데이터 기간에 정해진 답은 없지만, 경험적으로 상승장, 하락장, 대폭락 등 여러 케이스의 정보가 담겨있을 정도로 긴 시계열이 좋다. \n","따라서 애플(AAPL) 주식 데이터를 2013년부터 2023년까지 수집하여 훈련 데이터로 사용하고, 2024년 1월부터 4월까지의 데이터를 테스트 데이터로 사용할 것이다. 이를 위해 야후 파이낸스(yfinance) 라이브러리를 사용한다.그리고 불러온 데이터에 이동 평균과 RSI 지표를 추가하여 피처를 생성하고, 결측값을 제거한다.\n","\n","그 다음으로는 피처와 타깃 변수를 설정한다. 주식 데이터에서 Open, High, Low, Close, Volume, SMA_10, RSI 피처를 사용하고, 타깃 변수는 다음 날의 주가가 상승하는지 여부로 설정한다. 그리고 피처 데이터를 표준화하여 모델의 학습 효율을 높인다.머신러닝 실습에서 이미 진행한 부분이기 때문에 자세한 설명은 생략한다."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T06:27:55.371254Z","iopub.status.busy":"2024-05-15T06:27:55.370789Z","iopub.status.idle":"2024-05-15T06:27:55.494203Z","shell.execute_reply":"2024-05-15T06:27:55.493090Z","shell.execute_reply.started":"2024-05-15T06:27:55.371221Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n"]}],"source":["\n","# 데이터 불러오기 (2013년부터 2023년까지 훈련 데이터, 2024년 1월부터 4월까지 테스트 데이터)\n","train_data = yf.download('AAPL', start='2013-01-01', end='2024-01-01')\n","test_data = yf.download('AAPL', start='2024-01-01', end='2024-05-01')\n","\n","# 데이터 전처리 및 피처 엔지니어링\n","for data in [train_data, test_data]:\n","    data['SMA_10'] = data['Close'].rolling(window=10).mean()\n","    data['RSI'] = ta.momentum.RSIIndicator(data['Close']).rsi()\n","    data.dropna(inplace=True)\n","\n","# 피처와 타깃 변수 설정\n","features = ['Open', 'High', 'Low', 'Close', 'Volume', 'SMA_10', 'RSI']\n","\n","X_train = train_data[features].values\n","y_train = (train_data['Close'].shift(-1) > train_data['Close']).astype(int).values[:-1]\n","X_train = X_train[:-1]\n","\n","X_test = test_data[features].values\n","y_test = (test_data['Close'].shift(-1) > test_data['Close']).astype(int).values[:-1]\n","X_test = X_test[:-1]\n","\n","# 데이터 정규화\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T06:39:15.433189Z","iopub.status.busy":"2024-05-15T06:39:15.432709Z","iopub.status.idle":"2024-05-15T06:39:15.440646Z","shell.execute_reply":"2024-05-15T06:39:15.439438Z","shell.execute_reply.started":"2024-05-15T06:39:15.433152Z"},"trusted":true},"outputs":[],"source":["# 트랜스포머 모델 정의\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    # 멀티헤드 어텐션\n","    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n","    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n","    res = x + inputs\n","\n","    # 피드포워드 네트워크\n","    x = tf.keras.layers.Dense(ff_dim, activation=\"relu\")(res)\n","    x = tf.keras.layers.Dense(inputs.shape[-1])(x)\n","    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n","    return x + res"]},{"cell_type":"markdown","metadata":{},"source":["### 트랜스포머 모델 정의\n","\n","트랜스포머 모델은 자연어 처리 분야에서 처음 도입되었으나, 시계열 데이터 분석에도 매우 효과적이다. 트랜스포머 모델의 핵심 구성 요소는 멀티헤드 어텐션이다.\n","\n","#### 어텐션 메커니즘\n","먼저 어텐션 메커니즘에 대해 이해하는 것이 중요하다. 어텐션 메커니즘은 입력 데이터의 중요한 부분에 가중치를 부여하여 모델이 학습할 때 더 중요한 정보에 집중할 수 있도록 돕는다. 어텐션 메커니즘은 주로 다음과 같은 단계로 이루어진다:\n","\n","1. 쿼리(Query), 키(Key), 값(Value): 입력 데이터는 쿼리, 키, 값 세 가지로 변환된다.\n","2. 어텐션 스코어(Attention Score): 쿼리와 키의 내적을 계산하여 어텐션 스코어를 구한다.\n","3. 어텐션 가중치(Attention Weight): 어텐션 스코어를 소프트맥스 함수에 입력하여 가중치를 구한다.\n","4. 어텐션 출력(Attention Output): 어텐션 가중치와 값의 가중 합을 구하여 최종 출력을 계산한다.\n","5. 어텐션 메커니즘의 주요 아이디어는 중요한 부분에 더 집중하고, 덜 중요한 부분은 덜 집중하는 것이다.\n","\n","#### 멀티헤드 어텐션\n","**멀티헤드 어텐션(Multi-Head Attention)**은 여러 개의 어텐션 메커니즘을 병렬로 사용하여 다양한 표현을 학습할 수 있도록 한다. 각 어텐션 헤드는 서로 다른 부분에 주의를 기울이며, 이를 통해 모델이 더 풍부한 표현을 학습할 수 있다.\n","\n","멀티헤드 어텐션의 주요 구성 요소는 다음과 같다:\n","\n","- 헤드(Head): 각 헤드는 독립적인 어텐션 메커니즘을 가지고 있다.\n","- 병렬 계산(Parallel Computation): 여러 헤드를 병렬로 계산하여 다양한 패턴을 동시에 학습한다.\n","- 결합(Concatenation): 각 헤드의 출력을 결합하여 최종 출력을 만든다.\n","멀티헤드 어텐션은 다양한 패턴을 동시에 학습할 수 있게 하여, 모델의 성능을 크게 향상시킨다.\n","\n","아래 코드에서, 멀티헤드 어텐션은 tf.keras.layers.MultiHeadAttention 레이어를 통해 구현된다. 이 레이어는 입력 데이터의 여러 부분에 주의를 기울여 다양한 패턴을 학습할 수 있도록 한다.\n","\n","#### 트랜스포머 인코더 정의\n","트랜스포머 인코더는 멀티헤드 어텐션과 피드포워드 네트워크로 구성된다. 인코더는 입력 데이터를 여러 번 처리하여 최종 출력을 생성한다. 각 레이어는 다음과 같이 구성된다:\n","\n","1. 멀티헤드 어텐션: 입력 데이터의 여러 부분에 동시에 주의를 기울여 중요한 패턴을 학습한다.\n","2. LayerNormalization: 각 레이어의 출력을 정규화하여 학습을 안정화하고 속도를 높인다.\n","3. 피드포워드 네트워크: 각 타임스텝의 출력을 더 복잡한 특징으로 변환한다.\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n","    inputs = tf.keras.layers.Input(shape=input_shape)\n","    x = inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = tf.keras.layers.Flatten()(x)\n","    for dim in mlp_units:\n","        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n","        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n","    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","    return tf.keras.Model(inputs, outputs)"]},{"cell_type":"markdown","metadata":{},"source":["\n","#### 모델 빌드 함수 정의\n","이제 트랜스포머 인코더를 여러 층 쌓아 트랜스포머 모델을 구성한다. 또한, Flatten 레이어와 MLP(Multi-Layer Perceptron) 레이어를 추가하여 최종 출력을 생성한다. MLP 레이어에 드롭아웃을 적용하여 과적합을 방지한다.\n","\n","#### 응용 방법\n","트랜스포머 모델을 사용하여 시계열 데이터를 분석할 때, 다음과 같은 방법으로 모델의 성능을 최적화하고 분석 결과를 향상시킬 수 있다.\n","\n","첫째, 다양한 하이퍼파라미터를 조정하는 것이 중요하다. 예를 들어, 멀티헤드 어텐션의 헤드 수를 조정함으로써 모델이 데이터를 바라보는 관점을 다양화할 수 있다. 헤드 수를 늘리면 모델이 더 다양한 패턴을 학습할 수 있지만, 계산 비용이 증가한다는 점을 염두에 두어야 한다. 또한, 피드포워드 네트워크의 차원을 조정하여 모델의 복잡성을 조절할 수 있다. 피드포워드 네트워크의 차원을 늘리면 모델이 더 복잡한 패턴을 학습할 수 있지만, 과적합의 위험도 증가한다. 마지막으로, 드롭아웃 비율을 조정하여 모델의 일반화 능력을 향상시킬 수 있다. 드롭아웃 비율을 적절히 조정하면 과적합을 방지하고 모델의 성능을 안정화할 수 있다.\n","\n","둘째, 데이터 전처리를 통해 모델의 성능을 향상시킬 수 있다. 입력 데이터를 정규화하여 데이터의 스케일을 맞추면 모델이 더 빠르고 안정적으로 학습할 수 있다. 또한, 결측값을 처리하여 데이터의 완전성을 확보하는 것도 중요하다. 결측값이 있는 데이터를 그대로 사용할 경우, 모델의 성능이 저하될 수 있다. 이를 방지하기 위해 결측값을 적절히 처리하는 방법을 고려해야 한다.\n","\n","셋째, 모델 튜닝을 통해 최적의 하이퍼파라미터를 찾는 과정이 필요하다. 교차 검증을 사용하면 데이터의 특정 부분에만 의존하지 않고 모델의 성능을 평가할 수 있다. 이를 통해 모델이 일반화된 성능을 가지도록 도울 수 있다. 또한, 그리드 서치를 통해 다양한 하이퍼파라미터 조합을 테스트하여 최적의 조합을 찾을 수 있다. 이 과정을 통해 모델의 성능을 최대화할 수 있다.\n","\n","트랜스포머 모델은 복잡한 시계열 패턴을 효과적으로 학습할 수 있어, 다양한 금융 데이터 분석에 유용하게 사용될 수 있다. 이러한 응용 방법을 통해 트랜스포머 모델을 최적화하고, 이를 바탕으로 다양한 투자 전략을 개발할 수 있다."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T05:59:03.006083Z","iopub.status.busy":"2024-05-15T05:59:03.005651Z","iopub.status.idle":"2024-05-15T05:59:03.305679Z","shell.execute_reply":"2024-05-15T05:59:03.304786Z","shell.execute_reply.started":"2024-05-15T05:59:03.006051Z"},"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,169</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ multi_head_atten… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,169</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ multi_head_atten… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,169</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ multi_head_atten… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,169</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ multi_head_atten… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n","│                     │                   │            │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │      \u001b[38;5;34m7,169\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m2\u001b[0m │ multi_head_atten… │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │          \u001b[38;5;34m8\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m5\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m2\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │      \u001b[38;5;34m7,169\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m2\u001b[0m │ multi_head_atten… │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │          \u001b[38;5;34m8\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m5\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m2\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │      \u001b[38;5;34m7,169\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m2\u001b[0m │ multi_head_atten… │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │          \u001b[38;5;34m8\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m5\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m2\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │      \u001b[38;5;34m7,169\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n","│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m2\u001b[0m │ multi_head_atten… │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │          \u001b[38;5;34m8\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m5\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m2\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n","│                     │                   │            │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,897</span> (116.79 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,897\u001b[0m (116.79 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,897</span> (116.79 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,897\u001b[0m (116.79 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["# 모델 생성\n","input_shape = (X_train.shape[1], 1)\n","X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","model = build_model(\n","    input_shape,\n","    head_size=256,\n","    num_heads=4,\n","    ff_dim=4,\n","    num_transformer_blocks=4,\n","    mlp_units=[128],\n","    mlp_dropout=0.4,\n","    dropout=0.25\n",")\n","\n","model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"accuracy\"])\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{},"source":["#### 모델 컴파일 및 학습\n","트랜스포머 모델을 구축하고, 컴파일한다. 손실 함수로는 binary_crossentropy를 사용하고, 최적화 도구로 Adam 옵티마이저를 사용한다. 200 에포크 동안 모델을 학습시키고, 학습 곡선을 그려서 모델의 학습 상태를 모니터링한다."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T06:02:10.309599Z","iopub.status.busy":"2024-05-15T06:02:10.309134Z","iopub.status.idle":"2024-05-15T06:03:50.553627Z","shell.execute_reply":"2024-05-15T06:03:50.552473Z","shell.execute_reply.started":"2024-05-15T06:02:10.309561Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - accuracy: 0.5196 - loss: 0.6962 - val_accuracy: 0.5082 - val_loss: 0.6960\n","Epoch 2/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5011 - loss: 0.7005 - val_accuracy: 0.5118 - val_loss: 0.6957\n","Epoch 3/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5045 - loss: 0.7007 - val_accuracy: 0.5136 - val_loss: 0.6955\n","Epoch 4/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5059 - loss: 0.7030 - val_accuracy: 0.5191 - val_loss: 0.6948\n","Epoch 5/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5085 - loss: 0.7001 - val_accuracy: 0.5045 - val_loss: 0.6945\n","Epoch 6/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.4871 - loss: 0.7014 - val_accuracy: 0.5082 - val_loss: 0.6940\n","Epoch 7/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5207 - loss: 0.6974 - val_accuracy: 0.5154 - val_loss: 0.6938\n","Epoch 8/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5023 - loss: 0.6992 - val_accuracy: 0.5154 - val_loss: 0.6936\n","Epoch 9/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.5492 - loss: 0.6915 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 10/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5066 - loss: 0.6987 - val_accuracy: 0.5172 - val_loss: 0.6932\n","Epoch 11/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5174 - loss: 0.6990 - val_accuracy: 0.5172 - val_loss: 0.6936\n","Epoch 12/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5057 - loss: 0.6978 - val_accuracy: 0.5172 - val_loss: 0.6933\n","Epoch 13/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5206 - loss: 0.6940 - val_accuracy: 0.5172 - val_loss: 0.6938\n","Epoch 14/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5341 - loss: 0.6929 - val_accuracy: 0.5172 - val_loss: 0.6931\n","Epoch 15/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5303 - loss: 0.6970 - val_accuracy: 0.5172 - val_loss: 0.6933\n","Epoch 16/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5389 - loss: 0.6952 - val_accuracy: 0.5172 - val_loss: 0.6933\n","Epoch 17/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5040 - loss: 0.6981 - val_accuracy: 0.5172 - val_loss: 0.6929\n","Epoch 18/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5349 - loss: 0.6935 - val_accuracy: 0.5172 - val_loss: 0.6937\n","Epoch 19/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5212 - loss: 0.6947 - val_accuracy: 0.5172 - val_loss: 0.6939\n","Epoch 20/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5120 - loss: 0.6926 - val_accuracy: 0.5172 - val_loss: 0.6935\n","Epoch 21/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5329 - loss: 0.6929 - val_accuracy: 0.5172 - val_loss: 0.6933\n","Epoch 22/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5381 - loss: 0.6904 - val_accuracy: 0.5172 - val_loss: 0.6935\n","Epoch 23/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5355 - loss: 0.6942 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 24/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.5325 - loss: 0.6928 - val_accuracy: 0.5172 - val_loss: 0.6936\n","Epoch 25/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5194 - loss: 0.6966 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 26/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5203 - loss: 0.6941 - val_accuracy: 0.5172 - val_loss: 0.6933\n","Epoch 27/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5380 - loss: 0.6900 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 28/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5240 - loss: 0.6940 - val_accuracy: 0.5172 - val_loss: 0.6932\n","Epoch 29/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5119 - loss: 0.6982 - val_accuracy: 0.5172 - val_loss: 0.6933\n","Epoch 30/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5314 - loss: 0.6973 - val_accuracy: 0.5172 - val_loss: 0.6936\n","Epoch 31/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5206 - loss: 0.6963 - val_accuracy: 0.5172 - val_loss: 0.6931\n","Epoch 32/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5146 - loss: 0.6972 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 33/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5248 - loss: 0.6944 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 34/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5239 - loss: 0.6957 - val_accuracy: 0.5172 - val_loss: 0.6937\n","Epoch 35/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5219 - loss: 0.6943 - val_accuracy: 0.5172 - val_loss: 0.6933\n","Epoch 36/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5121 - loss: 0.6956 - val_accuracy: 0.5172 - val_loss: 0.6938\n","Epoch 37/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5269 - loss: 0.6942 - val_accuracy: 0.5172 - val_loss: 0.6939\n","Epoch 38/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5120 - loss: 0.6901 - val_accuracy: 0.5172 - val_loss: 0.6935\n","Epoch 39/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5363 - loss: 0.6906 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 40/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5090 - loss: 0.6940 - val_accuracy: 0.5172 - val_loss: 0.6935\n","Epoch 41/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5145 - loss: 0.6952 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 42/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5266 - loss: 0.6931 - val_accuracy: 0.5172 - val_loss: 0.6930\n","Epoch 43/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5056 - loss: 0.6948 - val_accuracy: 0.5172 - val_loss: 0.6930\n","Epoch 44/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5306 - loss: 0.6910 - val_accuracy: 0.5172 - val_loss: 0.6934\n","Epoch 45/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5205 - loss: 0.6964 - val_accuracy: 0.5172 - val_loss: 0.6932\n","Epoch 46/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.4921 - loss: 0.6943 - val_accuracy: 0.5172 - val_loss: 0.6933\n","Epoch 47/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5277 - loss: 0.6927 - val_accuracy: 0.5172 - val_loss: 0.6931\n","Epoch 48/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5289 - loss: 0.6959 - val_accuracy: 0.5172 - val_loss: 0.6929\n","Epoch 49/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5156 - loss: 0.6952 - val_accuracy: 0.5172 - val_loss: 0.6929\n","Epoch 50/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5339 - loss: 0.6910 - val_accuracy: 0.5172 - val_loss: 0.6930\n","Epoch 51/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5495 - loss: 0.6900 - val_accuracy: 0.5172 - val_loss: 0.6930\n","Epoch 52/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5270 - loss: 0.6918 - val_accuracy: 0.5172 - val_loss: 0.6929\n","Epoch 53/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5466 - loss: 0.6893 - val_accuracy: 0.5172 - val_loss: 0.6929\n","Epoch 54/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.4907 - loss: 0.7017 - val_accuracy: 0.5172 - val_loss: 0.6927\n","Epoch 55/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5442 - loss: 0.6881 - val_accuracy: 0.5172 - val_loss: 0.6929\n","Epoch 56/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5510 - loss: 0.6863 - val_accuracy: 0.5172 - val_loss: 0.6927\n","Epoch 57/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5365 - loss: 0.6943 - val_accuracy: 0.5191 - val_loss: 0.6925\n","Epoch 58/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5094 - loss: 0.6926 - val_accuracy: 0.5227 - val_loss: 0.6926\n","Epoch 59/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5510 - loss: 0.6878 - val_accuracy: 0.5154 - val_loss: 0.6927\n","Epoch 60/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5155 - loss: 0.6917 - val_accuracy: 0.5172 - val_loss: 0.6928\n","Epoch 61/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5210 - loss: 0.6921 - val_accuracy: 0.5209 - val_loss: 0.6926\n","Epoch 62/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5209 - loss: 0.6922 - val_accuracy: 0.5154 - val_loss: 0.6927\n","Epoch 63/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5277 - loss: 0.6949 - val_accuracy: 0.5172 - val_loss: 0.6930\n","Epoch 64/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5164 - loss: 0.6960 - val_accuracy: 0.5172 - val_loss: 0.6929\n","Epoch 65/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5353 - loss: 0.6918 - val_accuracy: 0.5172 - val_loss: 0.6927\n","Epoch 66/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5247 - loss: 0.6917 - val_accuracy: 0.5245 - val_loss: 0.6925\n","Epoch 67/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5482 - loss: 0.6906 - val_accuracy: 0.5245 - val_loss: 0.6925\n","Epoch 68/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.5441 - loss: 0.6911 - val_accuracy: 0.5227 - val_loss: 0.6925\n","Epoch 69/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.4906 - loss: 0.7005 - val_accuracy: 0.5209 - val_loss: 0.6924\n","Epoch 70/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.5182 - loss: 0.6954 - val_accuracy: 0.5281 - val_loss: 0.6924\n","Epoch 71/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5104 - loss: 0.6952 - val_accuracy: 0.5209 - val_loss: 0.6924\n","Epoch 72/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5262 - loss: 0.6917 - val_accuracy: 0.5318 - val_loss: 0.6925\n","Epoch 73/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5382 - loss: 0.6918 - val_accuracy: 0.5318 - val_loss: 0.6925\n","Epoch 74/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5323 - loss: 0.6913 - val_accuracy: 0.5172 - val_loss: 0.6926\n","Epoch 75/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5263 - loss: 0.6926 - val_accuracy: 0.5263 - val_loss: 0.6924\n","Epoch 76/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5158 - loss: 0.6939 - val_accuracy: 0.5191 - val_loss: 0.6925\n","Epoch 77/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5359 - loss: 0.6894 - val_accuracy: 0.5191 - val_loss: 0.6926\n","Epoch 78/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5280 - loss: 0.6919 - val_accuracy: 0.5281 - val_loss: 0.6924\n","Epoch 79/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5449 - loss: 0.6914 - val_accuracy: 0.5209 - val_loss: 0.6925\n","Epoch 80/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5046 - loss: 0.6957 - val_accuracy: 0.5209 - val_loss: 0.6924\n","Epoch 81/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5064 - loss: 0.6943 - val_accuracy: 0.5245 - val_loss: 0.6922\n","Epoch 82/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5100 - loss: 0.6973 - val_accuracy: 0.5245 - val_loss: 0.6925\n","Epoch 83/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5278 - loss: 0.6912 - val_accuracy: 0.5263 - val_loss: 0.6923\n","Epoch 84/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5395 - loss: 0.6939 - val_accuracy: 0.5299 - val_loss: 0.6925\n","Epoch 85/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5303 - loss: 0.6879 - val_accuracy: 0.5209 - val_loss: 0.6926\n","Epoch 86/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5465 - loss: 0.6913 - val_accuracy: 0.5227 - val_loss: 0.6925\n","Epoch 87/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5273 - loss: 0.6902 - val_accuracy: 0.5281 - val_loss: 0.6924\n","Epoch 88/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5188 - loss: 0.6888 - val_accuracy: 0.5318 - val_loss: 0.6924\n","Epoch 89/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.5297 - loss: 0.6935 - val_accuracy: 0.5299 - val_loss: 0.6924\n","Epoch 90/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5378 - loss: 0.6916 - val_accuracy: 0.5172 - val_loss: 0.6927\n","Epoch 91/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5235 - loss: 0.6912 - val_accuracy: 0.5281 - val_loss: 0.6923\n","Epoch 92/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5395 - loss: 0.6897 - val_accuracy: 0.5299 - val_loss: 0.6924\n","Epoch 93/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5277 - loss: 0.6934 - val_accuracy: 0.5263 - val_loss: 0.6925\n","Epoch 94/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5516 - loss: 0.6873 - val_accuracy: 0.5299 - val_loss: 0.6924\n","Epoch 95/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5400 - loss: 0.6929 - val_accuracy: 0.5172 - val_loss: 0.6921\n","Epoch 96/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5316 - loss: 0.6929 - val_accuracy: 0.5209 - val_loss: 0.6922\n","Epoch 97/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5331 - loss: 0.6912 - val_accuracy: 0.5209 - val_loss: 0.6921\n","Epoch 98/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5245 - loss: 0.6914 - val_accuracy: 0.5281 - val_loss: 0.6922\n","Epoch 99/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5204 - loss: 0.6927 - val_accuracy: 0.5281 - val_loss: 0.6924\n","Epoch 100/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.5308 - loss: 0.6917 - val_accuracy: 0.5299 - val_loss: 0.6923\n","Epoch 101/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5311 - loss: 0.6916 - val_accuracy: 0.5336 - val_loss: 0.6922\n","Epoch 102/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5470 - loss: 0.6911 - val_accuracy: 0.5227 - val_loss: 0.6925\n","Epoch 103/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5403 - loss: 0.6929 - val_accuracy: 0.5245 - val_loss: 0.6926\n","Epoch 104/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5251 - loss: 0.6938 - val_accuracy: 0.5263 - val_loss: 0.6922\n","Epoch 105/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.5370 - loss: 0.6915 - val_accuracy: 0.5390 - val_loss: 0.6923\n","Epoch 106/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5148 - loss: 0.6944 - val_accuracy: 0.5172 - val_loss: 0.6926\n","Epoch 107/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5310 - loss: 0.6896 - val_accuracy: 0.5299 - val_loss: 0.6928\n","Epoch 108/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5297 - loss: 0.6893 - val_accuracy: 0.5263 - val_loss: 0.6925\n","Epoch 109/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5210 - loss: 0.6957 - val_accuracy: 0.5263 - val_loss: 0.6926\n","Epoch 110/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5551 - loss: 0.6863 - val_accuracy: 0.5299 - val_loss: 0.6923\n","Epoch 111/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5255 - loss: 0.6941 - val_accuracy: 0.5354 - val_loss: 0.6923\n","Epoch 112/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5101 - loss: 0.6956 - val_accuracy: 0.5336 - val_loss: 0.6924\n","Epoch 113/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5360 - loss: 0.6912 - val_accuracy: 0.5372 - val_loss: 0.6924\n","Epoch 114/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5338 - loss: 0.6914 - val_accuracy: 0.5354 - val_loss: 0.6924\n","Epoch 115/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5054 - loss: 0.6919 - val_accuracy: 0.5245 - val_loss: 0.6927\n","Epoch 116/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5154 - loss: 0.6957 - val_accuracy: 0.5372 - val_loss: 0.6923\n","Epoch 117/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5269 - loss: 0.6910 - val_accuracy: 0.5281 - val_loss: 0.6924\n","Epoch 118/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5125 - loss: 0.6926 - val_accuracy: 0.5263 - val_loss: 0.6925\n","Epoch 119/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5335 - loss: 0.6920 - val_accuracy: 0.5354 - val_loss: 0.6924\n","Epoch 120/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5282 - loss: 0.6901 - val_accuracy: 0.5354 - val_loss: 0.6924\n","Epoch 121/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.5165 - loss: 0.6930 - val_accuracy: 0.5281 - val_loss: 0.6923\n","Epoch 122/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5379 - loss: 0.6924 - val_accuracy: 0.5245 - val_loss: 0.6926\n","Epoch 123/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5529 - loss: 0.6893 - val_accuracy: 0.5263 - val_loss: 0.6924\n","Epoch 124/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5481 - loss: 0.6906 - val_accuracy: 0.5263 - val_loss: 0.6922\n","Epoch 125/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5316 - loss: 0.6923 - val_accuracy: 0.5172 - val_loss: 0.6922\n","Epoch 126/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5476 - loss: 0.6887 - val_accuracy: 0.5191 - val_loss: 0.6921\n","Epoch 127/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5152 - loss: 0.6925 - val_accuracy: 0.5209 - val_loss: 0.6922\n","Epoch 128/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5454 - loss: 0.6880 - val_accuracy: 0.5172 - val_loss: 0.6922\n","Epoch 129/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.5425 - loss: 0.6908 - val_accuracy: 0.5245 - val_loss: 0.6923\n","Epoch 130/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.5280 - loss: 0.6934 - val_accuracy: 0.5172 - val_loss: 0.6923\n","Epoch 131/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5236 - loss: 0.6899 - val_accuracy: 0.5336 - val_loss: 0.6924\n","Epoch 132/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.5279 - loss: 0.6898 - val_accuracy: 0.5154 - val_loss: 0.6923\n","Epoch 133/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5144 - loss: 0.6913 - val_accuracy: 0.5172 - val_loss: 0.6923\n","Epoch 134/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5392 - loss: 0.6894 - val_accuracy: 0.5136 - val_loss: 0.6923\n","Epoch 135/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5423 - loss: 0.6876 - val_accuracy: 0.5227 - val_loss: 0.6923\n","Epoch 136/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5385 - loss: 0.6914 - val_accuracy: 0.5154 - val_loss: 0.6922\n","Epoch 137/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5459 - loss: 0.6884 - val_accuracy: 0.5245 - val_loss: 0.6922\n","Epoch 138/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5221 - loss: 0.6935 - val_accuracy: 0.5281 - val_loss: 0.6922\n","Epoch 139/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.5393 - loss: 0.6880 - val_accuracy: 0.5245 - val_loss: 0.6924\n","Epoch 140/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.5346 - loss: 0.6881 - val_accuracy: 0.5118 - val_loss: 0.6922\n","Epoch 141/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.5186 - loss: 0.6936 - val_accuracy: 0.5136 - val_loss: 0.6923\n","Epoch 142/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5273 - loss: 0.6900 - val_accuracy: 0.5263 - val_loss: 0.6923\n","Epoch 143/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5314 - loss: 0.6914 - val_accuracy: 0.5263 - val_loss: 0.6923\n","Epoch 144/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.5360 - loss: 0.6909 - val_accuracy: 0.5209 - val_loss: 0.6923\n","Epoch 145/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5287 - loss: 0.6901 - val_accuracy: 0.5245 - val_loss: 0.6924\n","Epoch 146/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.5233 - loss: 0.6939 - val_accuracy: 0.5227 - val_loss: 0.6924\n","Epoch 147/200\n","\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5068 - loss: 0.6945 - val_accuracy: 0.5227 - val_loss: 0.6923\n","Epoch 148/200\n","\u001b[1m13/69\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5484 - loss: 0.6856"]}],"source":["\n","# 모델 학습 (에포크 수 증가)\n","history = model.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=32)\n","\n","# 학습 곡선 시각화\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Loss Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['accuracy'], label='Train Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Accuracy Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### 학습 곡선 시각화\n","학습 곡선을 통해 과적합 여부를 확인할 수 있다. 이 예제에서는 검증 손실이 거의 일정하므로 과적합이 의심된다. 정규화 기법을 추가하거나 모델의 복잡도를 조정하여 이를 개선할 수 있다."]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T06:05:38.305127Z","iopub.status.busy":"2024-05-15T06:05:38.304680Z","iopub.status.idle":"2024-05-15T06:05:38.590587Z","shell.execute_reply":"2024-05-15T06:05:38.589528Z","shell.execute_reply.started":"2024-05-15T06:05:38.305092Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","훈련 데이터 정확도: 0.53\n","훈련 데이터 혼동 행렬:\n","[[707 599]\n"," [698 750]]\n","\n","훈련 데이터 분류 리포트:\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.54      0.52      1306\n","           1       0.56      0.52      0.54      1448\n","\n","    accuracy                           0.53      2754\n","   macro avg       0.53      0.53      0.53      2754\n","weighted avg       0.53      0.53      0.53      2754\n","\n"]}],"source":["\n","# 훈련 데이터 결과\n","y_train_pred = model.predict(X_train)\n","threshold = y_train.mean()  # 훈련 데이터 비율로 트레시홀드 설정\n","y_train_pred_threshold = (y_train_pred > threshold).astype(int)\n","accuracy_train = accuracy_score(y_train, y_train_pred_threshold)\n","conf_matrix_train = confusion_matrix(y_train, y_train_pred_threshold)\n","print(f\"훈련 데이터 정확도: {accuracy_train:.2f}\")\n","print(\"훈련 데이터 혼동 행렬:\")\n","print(conf_matrix_train)\n","print(\"\\n훈련 데이터 분류 리포트:\")\n","print(classification_report(y_train, y_train_pred_threshold))\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T06:05:42.790345Z","iopub.status.busy":"2024-05-15T06:05:42.789897Z","iopub.status.idle":"2024-05-15T06:05:43.747133Z","shell.execute_reply":"2024-05-15T06:05:43.746233Z","shell.execute_reply.started":"2024-05-15T06:05:42.790311Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436ms/step\n","테스트 데이터 정확도: 0.68\n","테스트 데이터 혼동 행렬:\n","[[32  9]\n"," [13 15]]\n","\n","테스트 데이터 분류 리포트:\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.78      0.74        41\n","           1       0.62      0.54      0.58        28\n","\n","    accuracy                           0.68        69\n","   macro avg       0.67      0.66      0.66        69\n","weighted avg       0.68      0.68      0.68        69\n","\n"]}],"source":["\n","# 테스트 데이터 결과\n","y_pred = model.predict(X_test)\n","y_pred_threshold = (y_pred > threshold).astype(int)\n","accuracy_test = accuracy_score(y_test, y_pred_threshold)\n","conf_matrix_test = confusion_matrix(y_test, y_pred_threshold)\n","print(f\"테스트 데이터 정확도: {accuracy_test:.2f}\")\n","print(\"테스트 데이터 혼동 행렬:\")\n","print(conf_matrix_test)\n","print(\"\\n테스트 데이터 분류 리포트:\")\n","print(classification_report(y_test, y_pred_threshold))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T06:07:08.169923Z","iopub.status.busy":"2024-05-15T06:07:08.169502Z","iopub.status.idle":"2024-05-15T06:07:08.179652Z","shell.execute_reply":"2024-05-15T06:07:08.178649Z","shell.execute_reply.started":"2024-05-15T06:07:08.169890Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 데이터 클래스 분포:\n","1    1448\n","0    1306\n","Name: count, dtype: int64\n","\n","테스트 데이터 클래스 분포:\n","0    41\n","1    28\n","Name: count, dtype: int64\n"]}],"source":["# 훈련 데이터와 테스트 데이터의 클래스 분포 확인\n","print(\"훈련 데이터 클래스 분포:\")\n","print(pd.Series(y_train).value_counts())\n","print(\"\\n테스트 데이터 클래스 분포:\")\n","print(pd.Series(y_test).value_counts())\n"]},{"cell_type":"markdown","metadata":{},"source":["#### 모델 평가\n","훈련 데이터와 테스트 데이터에서의 성능을 평가한 결과, 훈련 데이터의 정확도는 53% 정도, 테스트 데이터의 정확도는 68% 정도이다. (위에서 확인한 분포로 보아서는) 이는 테스트 데이터가 상승장에 편향되어 있기 때문일 수 있다. 따라서 실제로 '정확도'가 아닌 수익률을 봐야 한다. 또한 벤치마크 지표가 필요하다. 예를 들어 buy and hold 전략이 벤치마크가 될 수 있고, S&P 500 지수 추종이 벤치마크가 될 수 있다.\n","\n","트랜스포머 모델을 사용한 주가 방향성 예측 실습을 통해, 시계열 데이터에서의 패턴 인식과 예측의 가능성을 확인할 수 있었다. 그러나 실전에서는 다양한 정규화 기법과 모델 튜닝이 필요하며, 최종적인 목표는 정확도가 아닌 투자 수익률임을 항상 염두에 두어야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":4}
